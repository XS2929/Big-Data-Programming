Assignment 9 - Spark Setup
 Submit Assignment
Due Tuesday by 9:30am  Points 10  Submitting a file upload
For this assignment your task is to download and setup Spark, and run the Java class that implements WordCount for Spark.

Download version 2.2.0 of Spark, that can be found here:

https://spark.apache.org/downloads.htmlLinks to an external site.

Select these options:

Version 2.2.0
Prebuilt for Hadoop version 2.7 and later
untar the downloaded file, and you should be ready to go.  Look over the documentation provided here:

https://spark.apache.org/docs/2.2.0/quick-start.htmlLinks to an external site.

for details on running the Spark shell (using Scala), and self-contained (compiled) applications, like Java.

 

For this assignment, once you have the Spark install setup, you'll do the following:

Grab the pom.xml file and WordCount.java files on Canvas (Files / Assignment 9)
Create a new project structure using the above pom.xml file and Java file.  You can use your existing project structure if you want (I find creating a new project structure easier, your mileage may vary). If you do use your existing project structure, update the pom.xml file to the versions in the new pom.xml file I've provided, add version and dependency info for Spark, and add the configuration filters section to the shade plugin description.
Build the uber JAR in the same way you did for your Hadoop map-reduce job(s).
Run the Spark WordCount app
In the directory where Spark was installed (in my environment, that is a path ending with:  spark-2.2.0-bin-hadoop2.7), run the command:
./bin/spark-submit
Arguments:
--class "com.refactorlabs.cs378.assign9.WordCount"
--master local[1]
location of your uber JAR
input file: README.md
output directory
For the output directory, I created a directory named "output" in the Spark installation directory, and then specified a subdirectory for the assignment: output/assign9. Just like Hadoop map-reduce, Spark expects to create the directory, and therefore you should delete that directory (output/assign9 in my example) before running your Spark job. The input file is the readme file that comes with the Spark install.

Artifacts to submit

Assignment9Output.txt - the output file generated by the WordCount app
 

Useful links:

https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html (Links to an external site.)Links to an external site.

https://spark.apache.org/docs/2.1.0/api/python/pyspark.html (Links to an external site.)Links to an external site.