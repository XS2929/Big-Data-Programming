Goals:

Compile a Java map-reduce program
Create a JAR file containing the map-reduce code, ready for upload to AWS or Hadoop cluster
Login to your Hadoop cloud service (if not using a local install)
Create sub-folders in your account for jar files, logs, inputs, and outputs. I recommend "jars", "logs", "data", and "output".
Upload the JAR file
Define a cluster and job steps, and run the map-reduce job
Examine files generated by the map-reduce job execution
Download the output
In this assignment you will learn how to start with Java source code and end with the output generated by that map-reduce job running on AWS (Amazon Web Services). The Java source code and a Maven pom.xml file are provided. Although you are not creating Java code in this assignment, I encourage you to set up the environment you will be using to edit and compile your Java code, and import the example code there. I recommend using an IDE such as Eclipse (open source) or Intellij IDEA Community Edition (free) or Student Edition (free). Also make sure that you have the capability to run the maven script (pom.xml) that can generate your JAR file for each assignment, as this will be a required artifact that you must submit for assignments. This allows Vivek and I to execute and evaluate your code.

Compiling Java code, building the uber JAR.

The Java source code for WordCount and the pom.xml file used to build are available under Files / Assignment 1. I recommend that you get the file assign1.tar. This file contains the Java source code for WordCount (and WordCountTest) and the pom.xml file. If you untar this file (command: tar xvf assign1.tar), the Java source files and pom.xml file will be in the appropriate directories for compiling with maven, and for creating a project in Eclipse or IntelliJ. When creating a project in either of these IDEs, simply import the pom.xml file to create the project.

Once you have untar'd assign1.tar, you can compile the Java code and create the uber JAR for upload to AWS. The command is:

mvn clean package
The generated JAR file is in the target subdirectory.

Using AWS:

Login to AWS Management Console, select the S3 service. (Note: You can move this and other service icons to the toolbar if you like.) Click on the bucket name. The display now shows the folders in the S3 bucket. Click the "Create Folder" button and create folders as desired. I recommend you create subfolders named: data, jars, logs, and output. You will use these folders for:

data - uploading data files you create to test your map-reduce program.
jars - uploading your JAR files
logs - capturing the logs of a map-reduce job execution - useful for debugging and understanding the execution of your map-reduce job
output - capturing the output of your map-reduce job
Now you're set up to upload your JAR file, define a cluster and job step, and run a map-reduce program. Click on the jars folder. Once in this directory, click on the Upload button. Find your JAR file and upload it to S3. If you have a data file for testing, you can upload it to the data folder in a similar manner. For this assignment, you should use the data file dataSet1.txt (available on Canvas) to produce the output to submit to Canvas.

Now let's define a cluster and job step to run your map-reduce program. Select the EMR ("Elastic MapReduce") service. You may notice several clusters listed there. If no clusters are visible, you'll see Welcome to Amazon Elastic MapReduce page. In either case, click the "Create Cluster" button.

If your page starts with "Create Cluster - Quick Options", click on the text "Go to advanced options" just to the right.

Step 1: Software Configuration:
Step type: Select "Custom JAR", and click the Configure button.
In the "Add Step" dialog box:
Name: the name of this step. I suggest something like Assign1Run1 (you'll likely have multiple steps on future assignments).
JAR Location: s3://yourBucketName/jars/bdp-0.1.jar
Arguments: This map reduce job takes three arguments:
Full pathname of the class: com.refactorlabs.cs378.assign1.WordCount
The input file: s3://yourBucketName/data/dataSet1.txt
The output folder: s3://yourBucketName/output/Assign1Run1
Action on failure: Leave as "Continue"
Click "Add".
Note: If the output folder already exists, the job will fail, so you need to specify a new folder each time for the output, or delete the output folder before running the job.
Vendor: Amazon Release: emr-5.8.0
Applications: Hadoop 2.7.3. Unselect another other software packages, as they only slow down the cluster startup process.
Edit software settings (optional): nothing to enter here
Add steps (optional): Here is where you define the details of the map-reduce job to run.
Auto-terminate: I recommend that you say "No" here. The reason being that this terminates your cluster when the step completes, whether or not your map-reduce job was successful. In most cases you'll want to keep the cluster up and running so you can add another step (after uploading a new JAR file that contains code changes, or running our map-reduce job on a different data set). Cluster startup is somewhat slow, and this will improve turn-around time when you are debugging your code. Also, the minimum charge for any job is one hour of whatever resources are used. So if you run for 10 minutes or 58 minutes, you are charged for one hour. If you need to re-run the job, it's easy to add a step and run that step (remember to delete the output folder if you reuse it in another step). In keeping the cluster running, you don't want to inadvertently leave it running for many hours and run up the charges on your account.
Step 2: Hardware Configuration:
Leave "Uniform Instance groups" selected
Network: no change
EC2 Subnet: no change
Root device EBS volume size: no change
Master - Click on the edit icon (a pencil) next to the Instance Type, "m3.xlarge". I recommend selecting "m1.medium", as this is sufficient for the jobs we will run, and uses minimal resources so the usage charge is minimized.
Core - Set the instance count to 0
Task - Set the instance count to 0
Step 3: General Options:
Cluster name: Give the cluster a name (for example, Assign1 Run1)
Logging: Enable logging, and enter the pathname of your log folder. Assuming you created a folder named "logs", the logging folder S3 location will look like: s3://yourBucketName/logs. Log files will be placed under a folder there with a unique name assigned to the mapreduce job by Hadoop.
Debugging: Unselect
Termination Protection: Unselect
Scale down behavior: No change.
Tags: Nothing to do here.
Additional options: Nothing to do here.
Bootstrap Actions: None.
Step 4: Security Options:
EC2 key pair : Select the EC2 key you created with your account (not required).
Permissions: Nothing to do here.
Finally, click on Create Cluster at the bottom right, and your cluster will start up. Once it has completed provisioning and run bootstrapping code, your map-reduce job will run. Assuming that your job is successful, you can examine the log files and download and examine the output.
Artifacts to submit:

Assignment1Build.zip or tar - all files (Java, pom.xml) in the directory structure required by maven and buildable with your pom.xml file.
Assignment1Code.zip - all files (Java) in a flat directory for easy inspection for grading
Assignment1Output.txt - output file containing the word counts